{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6a27fc-2392-45b7-a617-1103bb3a282e",
   "metadata": {},
   "source": [
    "1. #Create a Doc object from the file owlcreek.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73004f5a-0fe2-4227-a9a6-35eec0fab57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read the content of the file\n",
    "with open(\"owlcreek.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073a8368-9114-4a84-baef-eba6c4747405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. How many tokens are contained in the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728aedbc-3edc-40e8-8c4c-b88aebec4dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4835\n"
     ]
    }
   ],
   "source": [
    "# Count tokens\n",
    "num_tokens = len(doc)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce163e9-f7e5-48e5-9ad8-75b56d94d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. How many sentences are contained in the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89112b48-cf87-4931-8951-24154245830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 204\n"
     ]
    }
   ],
   "source": [
    "# Count the number of sentences\n",
    "num_sentences = len(list(doc.sents))\n",
    "print(f\"Number of sentences: {num_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da93cc-08ab-4145-a411-a6013e5f170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Print the second sentence in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "281ad84d-55c2-499b-bbe1-cbf6bedc20a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence:\n",
      "The man's hands were behind\n",
      "his back, the wrists bound with a cord.  \n"
     ]
    }
   ],
   "source": [
    "# Convert sentences generator to a list\n",
    "sentences = list(doc.sents)\n",
    "# Print the second sentence (index 1)\n",
    "if len(sentences) >= 2:\n",
    "    print(\"Second sentence:\")\n",
    "    print(sentences[1])\n",
    "else:\n",
    "    print(\"The document has fewer than 2 sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5a12c-f07c-486a-8e7b-06882d17f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 5. For each token in the sentence above, print its text, POS tag, dep tag and lemma.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df78c9f5-81ff-4ee9-8ab7-2c89f462f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence:\n",
      "The man's hands were behind\n",
      "his back, the wrists bound with a cord.  \n",
      "\n",
      "Token           POS        Dep             Lemma\n",
      "-------------------------------------------------------\n",
      "The             DET        det             the\n",
      "man             NOUN       poss            man\n",
      "'s              PART       case            's\n",
      "hands           NOUN       nsubj           hand\n",
      "were            AUX        ROOT            be\n",
      "behind          ADP        prep            behind\n",
      "\n",
      "               SPACE      dep             \n",
      "\n",
      "his             PRON       poss            his\n",
      "back            NOUN       pobj            back\n",
      ",               PUNCT      punct           ,\n",
      "the             DET        det             the\n",
      "wrists          NOUN       appos           wrist\n",
      "bound           VERB       acl             bind\n",
      "with            ADP        prep            with\n",
      "a               DET        det             a\n",
      "cord            NOUN       pobj            cord\n",
      ".               PUNCT      punct           .\n",
      "                SPACE      dep              \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check and process the second sentence\n",
    "if len(sentences) >= 2:\n",
    "    second_sentence = sentences[1]\n",
    "    print(f\"Second sentence:\\n{second_sentence}\\n\")\n",
    "    print(f\"{'Token':<15} {'POS':<10} {'Dep':<15} {'Lemma'}\")\n",
    "    print(\"-\" * 55)\n",
    "    for token in second_sentence:\n",
    "        print(f\"{token.text:<15} {token.pos_:<10} {token.dep_:<15} {token.lemma_}\")\n",
    "else:\n",
    "    print(\"The document has fewer than 2 sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71579026-46ef-4b80-8f61-c6380d5933a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79dfd14-6654-4b83-a9d7-284949da3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found occurrences: ['swimming vigorously', 'swimming vigorously']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def Swimming(text):\n",
    "    # Define the pattern to search for the phrase \"swimming vigorously\"\n",
    "    pattern = r\"swimming vigorously\"\n",
    "    \n",
    "    # Find all occurrences of the phrase in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Return the list of matches\n",
    "    return matches\n",
    "\n",
    "# Example text to test the matcher\n",
    "example_text = \"He was swimming vigorously and then started swimming vigorously again.\"\n",
    "\n",
    "# Call the function and print the result\n",
    "matches = Swimming(example_text)\n",
    "print(f\"Found occurrences: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778a222-96d0-4d5c-ab03-0210326cd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Print the text surrounding each found match.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e51472e-0850-48e6-9b18-6adbcb456532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: 'swimming vigorously'\n",
      "Surrounding text: ...He was swimming vigorously and then started swimming vig...\n",
      "--------------------------------------------------\n",
      "Match found: 'swimming vigorously'\n",
      "Surrounding text: ...g vigorously and then started swimming vigorously again....\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def Swimming(text, context_length=30):\n",
    "    # Define the pattern to search for the phrase \"swimming vigorously\"\n",
    "    pattern = r\"swimming vigorously\"\n",
    "    \n",
    "    # Find all occurrences of the phrase and get the start and end positions\n",
    "    matches = [(match.start(), match.end()) for match in re.finditer(pattern, text)]\n",
    "    \n",
    "    # Iterate over the matches and print the surrounding text\n",
    "    for start, end in matches:\n",
    "        # Get the surrounding text before and after the match\n",
    "        start_context = max(0, start - context_length)\n",
    "        end_context = min(len(text), end + context_length)\n",
    "        \n",
    "        # Extract the context around the match\n",
    "        surrounding_text = text[start_context:end_context]\n",
    "        \n",
    "        # Print the surrounding text with the match highlighted\n",
    "        print(f\"Match found: '{text[start:end]}'\")\n",
    "        print(f\"Surrounding text: ...{surrounding_text}...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example text to test the matcher\n",
    "example_text = \"He was swimming vigorously and then started swimming vigorously again.\"\n",
    "\n",
    "# Call the function and print the result\n",
    "Swimming(example_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
